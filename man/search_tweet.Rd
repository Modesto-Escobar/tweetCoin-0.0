\name{search_tweet}
\alias{search_tweet}
\title{Search tweets using a term}
\description{
Search and download the tweets using a term, name or hashtag.
}
\usage{
search_tweet(search, maxtweets = 300, 
              type = "recent", include_rts = TRUE,
              geocode = NULL, since_id = NULL, 
              parse = TRUE, token = NULL, 
              retryonratelimit = FALSE, 
              verbose = TRUE, 
              output_file_name = NULL)
}
\arguments{
  \item{search}{Query to be searched, used to filter and select tweets to return from Twitter's REST API.}
  \item{maxtweets}{Integer specifying the total number of desired tweets to return (Default = 100). See details for more information.}
  \item{type}{Character string specifying which type of search results to return from Twitter's REST API (Default = "recent", other valid types include type = "mixed" and type = "popular".}
  \item{include_rts}{Selects whether to include retweets in search results. Retweets are classified as any tweet generated by Twitter's built-in "retweet" (Default = TRUE).}
  \item{geocode}{Geographical limiter of the template "latitude,longitude,radius" (i.e., geocode = "37.78,-122.40,1mi").}
  \item{since_id}{Character, returns results with an ID less than selected, thus allowing to retrieve older data (from "since_id" to the past). Specially useful to repeated queries, as it allows to concadenate multiple interrupted searchs without data repetition.}
  \item{parse}{Indicates whether to return parsed data.frame if true, or nested list if false. By default, parse = TRUE in order to clean the data extracted from Twitter.}
  \item{token}{File containing our Twitter tokens (Check \code{\link[rtweet]{search_tweets}} for more info).}
  \item{retryonratelimit}{Indicates whether to wait and retry when rate limited. This argument is only relevant if the desired return ("maxtweets") exceeds the limit of requests >18,000 tweets (Defaults = FALSE).}
  \item{verbose}{Check whether or not to include output processing/retrieval messages (Default = TRUE).} 
  \item{output_file_name}{Name of the output writting file.}
}
\details{
Twitter API documentation recommends limiting searches to 10 keywords and operators. 

The extracted number of tweets returned might be less than what was specified. This can happen because (a) the search query did not return many results, (b) because user hitting rate limit for a given token, or (c) of recent activity (either more tweets, which affect pagination in returned results or deletion of tweets). To return more than 18,000 tweets in a single call, check the information in \code{\link[rtweet]{search_tweets}}.

The argument "since_id" is specially useful for large data returns that require multiple iterations interrupted by user time constraints. For searches exceeding 18,000 tweets, users are encouraged to take advantage of internal automation procedures for waiting on rate limits by setting "retryonratelimit" argument to TRUE. It some cases, it is possible that due to processing time and rate limits, retrieving several million tweets can take several hours or even days. In these cases, it would likely be useful to leverage "retryonratelimit" for sets of tweets and since_id to allow results to continue where previous efforts left off.

All the tweets are saved into a .csv file. If there is a file with the same name, it reads it, the extracted tweets are concadenated sequentially and overwrites it.
If the archive file name is not specified, a default name is created.
}
\value{
A .csv file containg all the tweets containing the desired term(s).
}
\author{
Jose L.A. Berrocal,  University of Salamanca. See \url{http://berrocal.usal.es/}.
}
\examples{
## Not Run:
## Example searching for tweets using the hashtag "#Rstudio":
# search_tweet("#Rstudio", maxtweets = 300, 
#              type = "recent", include_rts = TRUE,
#              geocode = NULL, max_id = NULL, 
#              parse = TRUE, token = NULL, 
#              retryonratelimit = FALSE, 
#              verbose = TRUE, 
#              output_file_name = NULL)
              
## And now the .csv file can be upload to our environment:
# Rstudio <- load_tweets(name= "#Rstudio", 
#              type = "search")
## (End run)
}