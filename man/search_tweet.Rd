\name{search_tweet}
\alias{search_tweet}
\title{Search tweets using a term}
\description{
Search and download the tweets using a term, name or hashtag.
}
\usage{
search_tweet(search, maxtweets = 300, 
              type = "recent", include_rts = TRUE,
              geocode = NULL, since_id = NULL, 
              parse = TRUE, token = NULL, 
              retryonratelimit = FALSE, 
              verbose = TRUE, 
              output_file_name = NULL,
              format = c("binary", "delimited"))
}
\arguments{
  \item{search}{Query to be searched, used to filter and select tweets to return from Twitter's REST API.}
  \item{maxtweets}{Integer specifying the total number of desired tweets to return (Default = 300). See details for more information.}
  \item{type}{Character string specifying which type of search results to return from Twitter's REST API (Default = "recent", other valid types include type = "mixed" and type = "popular".}
  \item{include_rts}{Selects whether to include retweets in search results. Retweets are classified as any tweet generated by Twitter's built-in "retweet" (Default = TRUE).}
  \item{geocode}{Geographical limiter of the template "latitude,longitude,radius" (i.e., geocode = "37.78,-122.40,1mi").}
  \item{since_id}{Character, returns results with an ID more than selected, thus allowing to retrieve newer data (from "since_id" to the future). Specially useful to repeated queries, as it allows to concadenate multiple interrupted searchs without data repetition.}
  \item{parse}{Indicates whether to return parsed data.frame if true, or nested list if false. By default, parse = TRUE in order to clean the data extracted from Twitter.}
  \item{token}{File containing our Twitter tokens (check \code{\link[tweetCoin]{credentials}} for more info).}
  \item{retryonratelimit}{Indicates whether to wait and retry when rate limited. This argument is only relevant if the desired return ("maxtweets") exceeds the limit of requests >18,000 tweets (Defaults = FALSE).}
  \item{verbose}{Check whether or not to include output processing/retrieval messages (Default = TRUE).} 
  \item{output_file_name}{Change the name of the output file.}
  \item{format}{Format of the output file, "binary" for binary format (.dat), "delimited" for delimited text file (.csv).}
}
\details{
Twitter API documentation recommends limiting searches to 10 keywords and operators. 

The extracted number of tweets returned might be less than what was specified. This can happen because (a) the search query did not return many results, (b) because user hitting rate limit for a given token, or (c) of recent activity (either more tweets, which affect pagination in returned results or deletion of tweets). To return more than 18,000 tweets in a single call, check the information in \code{\link[rtweet]{search_tweets}}.

The argument "since_id" is specially useful for large data returns that require multiple iterations interrupted by user time constraints. For searches exceeding 18,000 tweets, users are encouraged to take advantage of internal automation procedures for waiting on rate limits by setting "retryonratelimit" argument to TRUE. It some cases, it is possible that due to processing time and rate limits, retrieving several million tweets can take several hours or even days. In these cases, it would likely be useful to leverage "retryonratelimit" for sets of tweets and since_id to allow results to continue where previous efforts left off.

If there is already a file with the same name, it reads it, the extracted tweets are concadenated sequentially and it overwrites the file. If the archive file name is not specified, a default name is created, which is specially usefull with \code{\link[tweetCoin]{load_tweets}} function.
}
\value{
A .csv or .dat file containg all the tweets containing the desired term(s).
}
\author{
Jose L.A. Berrocal,  University of Salamanca. See \url{http://berrocal.usal.es/}.
}
\examples{
## Not Run:
## Example searching for tweets using the hashtag "#Rstudio" and saving in delimited format:
# search_tweet("#Rstudio", maxtweets = 300, 
#              type = "recent", include_rts = TRUE,
#              format = "delimited")
              
## And now the .csv file can be upload to our environment:
# Rstudio <- load_tweets(name= "#Rstudio", 
#             type = "search", format= "delimited")

## If you prefer to save it in binary:
# search_tweet("#Rstudio", maxtweets = 300, 
#               type = "recent", include_rts = TRUE,
#               format = "binary")

## And open the .dat file:
# biRstudio <- load_tweets(name= "#Rstudio", 
#              type = "search", format= "binary")

## (End run)
}